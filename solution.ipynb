{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "8dae3f798e3de614"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-08T04:05:53.057119Z",
     "start_time": "2026-01-08T04:05:53.047229Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, classification_report, fbeta_score, accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import optuna\n",
    "import statsmodels.api as sm\n",
    "from boruta import BorutaPy\n",
    "\n",
    "import custom_map"
   ],
   "outputs": [],
   "execution_count": 559
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(custom_map)"
   ],
   "id": "6b26b5db5d2c60ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data and feature engineering",
   "id": "121131a70010e491"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "data.info()"
   ],
   "id": "2691a56cef39f5af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target = \"stroke\"\n",
    "\n",
    "categorical_features = data.select_dtypes(['object']).columns.tolist()\n",
    "numerical_features = data.select_dtypes(['float64', 'int64']).columns.drop('id')\n",
    "categorical_features.append('stroke')\n",
    "\n",
    "print(categorical_features, '\\n', numerical_features)\n",
    "\n",
    "data.head()"
   ],
   "id": "463ad54d6139cad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data[numerical_features].hist(bins=30, figsize=(15, 10), color=\"teal\", edgecolor='black')\n",
    "plt.suptitle(\"Histogramy zmiennych numerycznych\", fontsize=16)\n",
    "plt.show()"
   ],
   "id": "61a7e0031277b739",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bmi_median = data['bmi'].median()\n",
    "bmi_mean = data['bmi'].mean()\n",
    "print(bmi_median, bmi_mean)\n",
    "\n",
    "data['bmi'] = data['bmi'].fillna(bmi_median)"
   ],
   "id": "6fc114b4fdc24314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nrows, ncols = 2, 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 9))\n",
    "axes = axes.flat\n",
    "\n",
    "for ax, col in zip(axes, categorical_features):\n",
    "    sizes = data[col].value_counts()\n",
    "    labels = sizes.index.astype(str)\n",
    "\n",
    "    ax.pie(sizes, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "    ax.set_title(col)\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "218adfb038318078",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_features.remove('stroke')\n",
    "data = pd.get_dummies(data, columns=categorical_features, drop_first=True, dtype=float)\n",
    "data = data.drop('id', axis=1)"
   ],
   "id": "2f2b3e887fbe79f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlation_data = data[numerical_features].corr()\n",
    "\n",
    "n_features = data[numerical_features].shape[1]\n",
    "n = data[numerical_features].size/data[numerical_features].shape[1]\n",
    "custom_map.cmap_pearson(n_features, n, 0.1)\n",
    "print(custom_map.cmap_pearson(n_features, n, 0.1))\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(correlation_data, annot=True, fmt=\".2f\", cmap=\"bajon\", vmin=-1, vmax=1) #tab20b\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "271963957020ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlation_data = data.corr()\n",
    "\n",
    "n_features = data[numerical_features].shape[1]\n",
    "n = data[numerical_features].size/data[numerical_features].shape[1]\n",
    "custom_map.cmap_pearson(n_features, n, 0.1)\n",
    "print(custom_map.cmap_pearson(n_features, n, 0.1))\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(correlation_data, annot=True, fmt=\".2f\", cmap=\"bajon\", vmin=-1, vmax=1) #tab20b\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "dea6d064ee49d997",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, shuffle=True)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "5bdb49ad0ebe8d12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature selection",
   "id": "652ec9d82144c9c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "boruta = BorutaPy(\n",
    "    estimator=rf,\n",
    "    n_estimators='auto',   # lub liczba\n",
    "    max_iter=100,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "boruta.fit(X_train, y_train)\n",
    "\n",
    "selected_mask = boruta.support_\n",
    "selected_features = np.where(selected_mask)[0]\n",
    "print(\"Wybrane cechy (indeksy):\", selected_features)\n",
    "print(\"Ranking:\", boruta.ranking_)"
   ],
   "id": "18dba6279078c87a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selected_indices = np.where(boruta.support_)[0]\n",
    "boruta_features = X_train.columns[selected_indices].tolist()\n",
    "\n",
    "boruta_features"
   ],
   "id": "898d72e6349b3185",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_features = data.corr()[target].sort_values(ascending=False)\n",
    "corr_features = corr_features[corr_features > custom_map.cmap_pearson(n_features, n, 0.1)['r_crit']].index.tolist()\n",
    "\n",
    "corr_features.remove('stroke')\n",
    "corr_features"
   ],
   "id": "9fab026249573ff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[corr_features].head()",
   "id": "932ca4bfa2aac37b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_chosen = data[corr_features].copy()\n",
    "X_scaled = scaler.fit_transform(X_chosen)\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    random_state=42,\n",
    "    n_init=20\n",
    ")\n",
    "\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "X_chosen[\"cluster\"] = clusters\n",
    "\n",
    "X_chosen[\"cluster\"].describe()"
   ],
   "id": "9d22c54d106c32ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train.info()",
   "id": "19eab7f2b5074a2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_idx = np.random.choice(len(X_train), size=int(X_train.size/20), replace=False)\n",
    "\n",
    "mi = mutual_info_regression(X_train.iloc[sample_idx], y_train.iloc[sample_idx])\n",
    "mi_df = pd.DataFrame({\"Feature\": X_train.columns, \"Mutual Information\": mi})\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.barh(X_train.columns, mi)\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Mutual Information')\n",
    "plt.show()\n",
    "mi_df\n",
    "\n"
   ],
   "id": "ecdf0fe1f088ca69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "names = X_train.columns\n",
    "# 1. Compute Spearman correlation and distance matrix\n",
    "# Assuming X is your dataframe of explanatory variables\n",
    "corr = spearmanr(X).correlation\n",
    "# Ensure the matrix is symmetric (sometimes float errors occur)\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# Convert correlation to a distance matrix\n",
    "dist_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(hierarchy.distance.squareform(dist_matrix))\n",
    "\n",
    "# 2. Visualize the Dendrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=names, ax=ax, leaf_rotation=90\n",
    ")\n",
    "ax.set_title(\"Hierarchical Clustering Dendrogram (Feature Redundancy)\")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 3. Select Features\n",
    "# Threshold '1' is common for 1 - abs(corr), but you can adjust based on the plot\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, t=1, criterion='distance')\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "\n",
    "# Keep only the first feature from each cluster\n",
    "selected_idx = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "mi_features = names[selected_idx]\n",
    "X_reduced = X_train.iloc[:, selected_idx]\n",
    "\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced features: {len(mi_features)}\")\n",
    "print(f\"Selected: {mi_features}\")"
   ],
   "id": "69fbfb995b1f2564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_vif(df: pd.DataFrame):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "    print(vif_data)"
   ],
   "id": "f7f76f3d486e7d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "calculate_vif(X_train.drop(columns=[\"age\", \"bmi\"]))",
   "id": "d455413601af7838",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vif_features = X_train.drop(columns=[\"age\", \"bmi\"]).columns.tolist()\n",
    "vif_features"
   ],
   "id": "dff9abb164f75053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=10000,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring=\"roc_auc\",\n",
    "    min_features_to_select=3\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "rfe_features = X_train.columns[rfecv.support_].tolist()\n",
    "ranking = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"rank\": rfecv.ranking_\n",
    "}).sort_values(\"rank\")\n",
    "\n",
    "print(\"Liczba wybranych cech:\", len(rfe_features))\n",
    "print(\"Wybrane cechy:\")\n",
    "print(rfe_features)"
   ],
   "id": "b168c5c12b7c928b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(\n",
    "    range(rfecv.min_features_to_select, len(rfecv.cv_results_[\"mean_test_score\"]) + rfecv.min_features_to_select),\n",
    "    rfecv.cv_results_[\"mean_test_score\"]\n",
    ")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"CV ROC AUC\")\n",
    "plt.title(\"RFECV performance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "14a439b566a98172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model training",
   "id": "29568911cc43eb02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## hard coding because of stochasticity\n",
    "\n",
    "all_features = [\"age\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\",\n",
    "                \"bmi\", \"gender_Male\", \"gender_Other\", \"ever_married_Yes\",\n",
    "                \"work_type_Never_worked\", \"work_type_Private\", \"work_type_Self-employed\",\n",
    "                \"work_type_children\", \"Residence_type_Urban\",\n",
    "                \"smoking_status_formerly smoked\", \"smoking_status_never smoked\",\n",
    "                \"smoking_status_smokes\"]\n",
    "\n",
    "boruta_features = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "\n",
    "corr_features = [\"age\", \"heart_disease\", \"avg_glucose_level\", \"hypertension\",\n",
    "                 \"ever_married_Yes\", \"smoking_status_formerly smoked\",\n",
    "                 \"work_type_Self-employed\"]\n",
    "\n",
    "mi_features = [\"age\", \"hypertension\", \"gender_Other\",\n",
    "               \"work_type_Private\", \"smoking_status_formerly smoked\"]\n",
    "\n",
    "rfe_features = [\"age\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\",\n",
    "                \"bmi\", \"work_type_Never_worked\", \"work_type_children\",\n",
    "                \"Residence_type_Urban\", \"smoking_status_never smoked\",\n",
    "                \"smoking_status_smokes\"]\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"all\": all_features,\n",
    "    \"boruta\": boruta_features,\n",
    "    \"correlation\": corr_features,\n",
    "    \"mi\": mi_features,\n",
    "    \"rfe\": rfe_features\n",
    "}"
   ],
   "id": "fa7f643080bb2f5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_all = X_train[all_features] # all features\n",
    "X_train_boruta = X_train[boruta_features].copy() # boruta selection\n",
    "X_train_corr = X_train[corr_features].copy() # cmap correlation\n",
    "X_train_mi = X_train[mi_features].copy() # correlation, mi & clustering\n",
    "X_train_rfe = X_train[rfe_features].copy() # rfecv\n",
    "\n",
    "data = {\n",
    "    \"Method\": [\n",
    "        \"All features\",\n",
    "        \"Boruta\",\n",
    "        \"Corelation\",\n",
    "        \"Mutual Information\",\n",
    "        \"RFE\"\n",
    "    ],\n",
    "    \"Feature quantity\": [\n",
    "        len(all_features),\n",
    "        len(boruta_features),\n",
    "        len(corr_features),\n",
    "        len(mi_features),\n",
    "        len(rfe_features)\n",
    "    ],\n",
    "    \"Feature name\": [\n",
    "        \", \".join(all_features),\n",
    "        \", \".join(boruta_features),\n",
    "        \", \".join(corr_features),\n",
    "        \", \".join(mi_features),\n",
    "        \", \".join(rfe_features)\n",
    "    ]\n",
    "}\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ],
   "id": "251fd8ee5cb3e1aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:42:12.810099Z",
     "start_time": "2026-01-08T03:42:12.795078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "storage_url = \"sqlite:///optuna_studies.db\"\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "MODELS = [\n",
    "    \"logreg\", \"knn\", \"svm\", \"gnb\", \"dt\",\n",
    "    \"rf\", \"ada\", \"gb\", \"extra\",\n",
    "    \"lgbm\", \"xgb\", \"cat\"\n",
    "]\n",
    "\n",
    "SMOTE_MODELS = {\"logreg\", \"knn\", \"svm\", \"gnb\"}"
   ],
   "id": "fbcfac68cebe52bb",
   "outputs": [],
   "execution_count": 542
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sanitize_params(model, params):\n",
    "    valid = model.get_params().keys()\n",
    "    return {k: v for k, v in params.items() if k in valid}\n"
   ],
   "id": "148f7c0d577ec3ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_model_base(name, pos_weight=None):\n",
    "\n",
    "    use_smote = name in SMOTE_MODELS\n",
    "\n",
    "    def smote():\n",
    "        return BorderlineSMOTE(\n",
    "            sampling_strategy=0.3,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"logreg\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            C=1.0,\n",
    "            class_weight=None if use_smote else \"balanced\",\n",
    "            max_iter=5000,\n",
    "            random_state=42\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"knn\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", KNeighborsClassifier(\n",
    "            n_neighbors=15,\n",
    "            weights=\"distance\"\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"svm\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", SVC(\n",
    "            C=1.0,\n",
    "            kernel=\"rbf\",\n",
    "            probability=True,\n",
    "            class_weight=None if use_smote else \"balanced\",\n",
    "            random_state=42\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"gnb\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", GaussianNB()))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"dt\":\n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=20,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"extra\":\n",
    "        return ExtraTreesClassifier(\n",
    "            n_estimators=500,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"ada\":\n",
    "        return AdaBoostClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"gb\":\n",
    "        return GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"lgbm\":\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"xgb\":\n",
    "        return XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=pos_weight,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"cat\":\n",
    "        return CatBoostClassifier(\n",
    "            iterations=500,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function=\"Logloss\",\n",
    "            auto_class_weights=\"Balanced\",\n",
    "            verbose=False,\n",
    "            random_seed=42\n",
    "        )\n",
    "\n",
    "    raise ValueError(name)"
   ],
   "id": "87dfaf6221b0606b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_model_search(trial, name):\n",
    "\n",
    "    use_smote = name in SMOTE_MODELS\n",
    "\n",
    "    def smote():\n",
    "        return BorderlineSMOTE(\n",
    "            sampling_strategy=trial.suggest_float(\"smote_ratio\", 0.2, 0.5),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"logreg\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", LogisticRegression(\n",
    "            C=trial.suggest_float(\"clf__C\", 1e-3, 10, log=True),\n",
    "            l1_ratio=trial.suggest_float(\"clf__l1_ratio\", 0.0, 1.0),\n",
    "            solver=\"saga\",\n",
    "            class_weight=None,\n",
    "            max_iter=5000,\n",
    "            random_state=42\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"knn\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", KNeighborsClassifier(\n",
    "            n_neighbors=trial.suggest_int(\"clf__n_neighbors\", 3, 25),\n",
    "            weights=trial.suggest_categorical(\"clf__weights\", [\"uniform\", \"distance\"])\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"svm\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", SVC(\n",
    "            C=trial.suggest_float(\"clf__C\", 1e-2, 10, log=True),\n",
    "            kernel=\"rbf\",\n",
    "            probability=True,\n",
    "            class_weight=None,\n",
    "            random_state=42\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"gnb\":\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "        if use_smote:\n",
    "            steps.append((\"smote\", smote()))\n",
    "        steps.append((\"clf\", GaussianNB(\n",
    "            var_smoothing=trial.suggest_float(\"clf__var_smoothing\", 1e-12, 1e-8, log=True)\n",
    "        )))\n",
    "        return ImbPipeline(steps)\n",
    "\n",
    "    if name == \"dt\":\n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 2, 15),\n",
    "            min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"extra\":\n",
    "        return ExtraTreesClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"ada\":\n",
    "        return AdaBoostClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"gb\":\n",
    "        return GradientBoostingClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 2, 5),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"lgbm\":\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            num_leaves=trial.suggest_int(\"num_leaves\", 16, 64),\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == \"xgb\":\n",
    "        return XGBClassifier(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    raise ValueError(name)"
   ],
   "id": "e8f212c269e8519f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective_catboost(trial, X, y):\n",
    "\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 200, 600),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"auto_class_weights\": \"Balanced\",\n",
    "        \"verbose\": False,\n",
    "        \"random_seed\": 42\n",
    "    }\n",
    "\n",
    "    f1s = []\n",
    "\n",
    "    for tr, va in cv.split(X, y):\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X.iloc[tr], y.iloc[tr])\n",
    "        preds = model.predict(X.iloc[va])\n",
    "        f1s.append(f1_score(y.iloc[va], preds))\n",
    "\n",
    "    return np.mean(f1s)"
   ],
   "id": "5a361ea26df9e814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T02:59:52.184341Z",
     "start_time": "2026-01-08T02:59:52.168608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_final(name, params):\n",
    "    model = get_model_base(name)\n",
    "    clean_params = sanitize_params(model, params)\n",
    "    model.set_params(**clean_params)\n",
    "    return model"
   ],
   "id": "3e879fb8f9db05c7",
   "outputs": [],
   "execution_count": 518
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T02:59:55.305127Z",
     "start_time": "2026-01-08T02:59:55.293098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_f1(model, X, y, threshold=0.5):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    return f1_score(y, y_pred)"
   ],
   "id": "75087f6d26b33e1c",
   "outputs": [],
   "execution_count": 519
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:30:02.249933Z",
     "start_time": "2026-01-08T03:30:02.240306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_best_f1_threshold(model, X, y, thresholds=np.linspace(0.01, 0.5, 50)):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = t\n",
    "\n",
    "    return best_thr, best_f1\n"
   ],
   "id": "97011bd281fe4220",
   "outputs": [],
   "execution_count": 532
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:32:16.075445Z",
     "start_time": "2026-01-08T03:32:12.481265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for feature_name, feature_list in FEATURE_SETS.items():\n",
    "\n",
    "    X_train_sel = X_train[feature_list]\n",
    "\n",
    "    for model_name in MODELS:\n",
    "\n",
    "        study_name = f\"{model_name}_{feature_name}_prob\"\n",
    "\n",
    "        with open(f\"models/{study_name}.pkl\", \"rb\") as f:\n",
    "            artifact = pickle.load(f)\n",
    "\n",
    "        model = artifact[\"model\"]\n",
    "\n",
    "        best_thr, best_f1 = find_best_f1_threshold(\n",
    "            model, X_train_sel, y_train\n",
    "        )\n",
    "\n",
    "        artifact = {\n",
    "            \"model\": model,\n",
    "            \"threshold\": best_thr,\n",
    "            \"f1_train\": best_f1\n",
    "        }\n",
    "\n",
    "        with open(f\"models/{study_name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(artifact, f)\n",
    "\n",
    "        print(\n",
    "            f\"SAVED {study_name} | \"\n",
    "            f\"thr={best_thr:.2f} | \"\n",
    "            f\"F1_train={best_f1:.4f}\"\n",
    "        )\n"
   ],
   "id": "dfaaec59c66d3b86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED logreg_all_prob | thr=0.50 | F1_train=0.2265\n",
      "SAVED knn_all_prob | thr=0.01 | F1_train=1.0000\n",
      "SAVED svm_all_prob | thr=0.14 | F1_train=0.5455\n",
      "SAVED gnb_all_prob | thr=0.37 | F1_train=0.1060\n",
      "SAVED dt_all_prob | thr=0.44 | F1_train=0.2641\n",
      "SAVED rf_all_prob | thr=0.50 | F1_train=0.3718\n",
      "SAVED ada_all_prob | thr=0.42 | F1_train=0.2869\n",
      "SAVED gb_all_prob | thr=0.27 | F1_train=0.9823\n",
      "SAVED extra_all_prob | thr=0.50 | F1_train=0.2632\n",
      "SAVED lgbm_all_prob | thr=0.50 | F1_train=0.5237\n",
      "SAVED xgb_all_prob | thr=0.29 | F1_train=0.8615\n",
      "SAVED cat_all_prob | thr=0.50 | F1_train=0.6746\n",
      "SAVED logreg_boruta_prob | thr=0.50 | F1_train=0.2230\n",
      "SAVED knn_boruta_prob | thr=0.01 | F1_train=0.4569\n",
      "SAVED svm_boruta_prob | thr=0.13 | F1_train=0.2782\n",
      "SAVED gnb_boruta_prob | thr=0.12 | F1_train=0.2437\n",
      "SAVED dt_boruta_prob | thr=0.46 | F1_train=0.2326\n",
      "SAVED rf_boruta_prob | thr=0.50 | F1_train=0.3449\n",
      "SAVED ada_boruta_prob | thr=0.32 | F1_train=0.2901\n",
      "SAVED gb_boruta_prob | thr=0.31 | F1_train=0.9671\n",
      "SAVED extra_boruta_prob | thr=0.50 | F1_train=0.2749\n",
      "SAVED lgbm_boruta_prob | thr=0.50 | F1_train=0.3180\n",
      "SAVED xgb_boruta_prob | thr=0.34 | F1_train=0.6404\n",
      "SAVED cat_boruta_prob | thr=0.50 | F1_train=0.5122\n",
      "SAVED logreg_correlation_prob | thr=0.50 | F1_train=0.1980\n",
      "SAVED knn_correlation_prob | thr=0.01 | F1_train=0.4585\n",
      "SAVED svm_correlation_prob | thr=0.15 | F1_train=0.3684\n",
      "SAVED gnb_correlation_prob | thr=0.15 | F1_train=0.2359\n",
      "SAVED dt_correlation_prob | thr=0.47 | F1_train=0.3272\n",
      "SAVED rf_correlation_prob | thr=0.49 | F1_train=0.4306\n",
      "SAVED ada_correlation_prob | thr=0.41 | F1_train=0.2950\n",
      "SAVED gb_correlation_prob | thr=0.28 | F1_train=0.9874\n",
      "SAVED extra_correlation_prob | thr=0.50 | F1_train=0.2951\n",
      "SAVED lgbm_correlation_prob | thr=0.50 | F1_train=0.3968\n",
      "SAVED xgb_correlation_prob | thr=0.33 | F1_train=0.9900\n",
      "SAVED cat_correlation_prob | thr=0.50 | F1_train=0.4987\n",
      "SAVED logreg_mi_prob | thr=0.50 | F1_train=0.2281\n",
      "SAVED knn_mi_prob | thr=0.01 | F1_train=0.3182\n",
      "SAVED svm_mi_prob | thr=0.13 | F1_train=0.2688\n",
      "SAVED gnb_mi_prob | thr=0.50 | F1_train=0.1115\n",
      "SAVED dt_mi_prob | thr=0.49 | F1_train=0.2526\n",
      "SAVED rf_mi_prob | thr=0.50 | F1_train=0.2840\n",
      "SAVED ada_mi_prob | thr=0.38 | F1_train=0.2783\n",
      "SAVED gb_mi_prob | thr=0.18 | F1_train=0.3987\n",
      "SAVED extra_mi_prob | thr=0.50 | F1_train=0.2569\n",
      "SAVED lgbm_mi_prob | thr=0.49 | F1_train=0.2647\n",
      "SAVED xgb_mi_prob | thr=0.20 | F1_train=0.4029\n",
      "SAVED cat_mi_prob | thr=0.50 | F1_train=0.2871\n",
      "SAVED logreg_rfe_prob | thr=0.50 | F1_train=0.2295\n",
      "SAVED knn_rfe_prob | thr=0.01 | F1_train=1.0000\n",
      "SAVED svm_rfe_prob | thr=0.14 | F1_train=0.3914\n",
      "SAVED gnb_rfe_prob | thr=0.50 | F1_train=0.1097\n",
      "SAVED dt_rfe_prob | thr=0.46 | F1_train=0.2409\n",
      "SAVED rf_rfe_prob | thr=0.50 | F1_train=0.2860\n",
      "SAVED ada_rfe_prob | thr=0.41 | F1_train=0.2816\n",
      "SAVED gb_rfe_prob | thr=0.29 | F1_train=0.9463\n",
      "SAVED extra_rfe_prob | thr=0.50 | F1_train=0.2421\n",
      "SAVED lgbm_rfe_prob | thr=0.50 | F1_train=0.5512\n",
      "SAVED xgb_rfe_prob | thr=0.12 | F1_train=1.0000\n",
      "SAVED cat_rfe_prob | thr=0.50 | F1_train=0.5793\n"
     ]
    }
   ],
   "execution_count": 536
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:32:18.824010Z",
     "start_time": "2026-01-08T03:32:18.015134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame(\n",
    "    index=MODELS,\n",
    "    columns=FEATURE_SETS.keys(),\n",
    "    dtype=float\n",
    ")\n",
    "\n",
    "for feature_name, feature_list in FEATURE_SETS.items():\n",
    "\n",
    "    X_test_sel = X_test[feature_list]\n",
    "\n",
    "    for model_name in MODELS:\n",
    "\n",
    "        study_name = f\"{model_name}_{feature_name}_prob\"\n",
    "\n",
    "        with open(f\"models/{study_name}.pkl\", \"rb\") as f:\n",
    "            artifact = pickle.load(f)\n",
    "\n",
    "        model = artifact[\"model\"]\n",
    "        best_thr = artifact[\"threshold\"]\n",
    "\n",
    "        proba_test = model.predict_proba(X_test_sel)[:, 1]\n",
    "        y_pred_test = (proba_test >= best_thr).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred_test)\n",
    "        results.loc[model_name, feature_name] = f1\n",
    "\n",
    "        print(\n",
    "            f\"{model_name}_{feature_name} | \"\n",
    "            f\"thr={best_thr:.2f} | \"\n",
    "            f\"F1_test={f1:.4f}\"\n",
    "        )\n"
   ],
   "id": "fec5fe0d210d7e36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg_all | thr=0.50 | F1_test=0.2331\n",
      "knn_all | thr=0.01 | F1_test=0.1319\n",
      "svm_all | thr=0.14 | F1_test=0.1101\n",
      "gnb_all | thr=0.37 | F1_test=0.1075\n",
      "dt_all | thr=0.44 | F1_test=0.2027\n",
      "rf_all | thr=0.50 | F1_test=0.2739\n",
      "ada_all | thr=0.42 | F1_test=0.2791\n",
      "gb_all | thr=0.27 | F1_test=0.2222\n",
      "extra_all | thr=0.50 | F1_test=0.2145\n",
      "lgbm_all | thr=0.50 | F1_test=0.2827\n",
      "xgb_all | thr=0.29 | F1_test=0.1923\n",
      "cat_all | thr=0.50 | F1_test=0.2432\n",
      "logreg_boruta | thr=0.50 | F1_test=0.2286\n",
      "knn_boruta | thr=0.01 | F1_test=0.2072\n",
      "svm_boruta | thr=0.13 | F1_test=0.3028\n",
      "gnb_boruta | thr=0.12 | F1_test=0.2869\n",
      "dt_boruta | thr=0.46 | F1_test=0.2137\n",
      "rf_boruta | thr=0.50 | F1_test=0.2538\n",
      "ada_boruta | thr=0.32 | F1_test=0.3140\n",
      "gb_boruta | thr=0.31 | F1_test=0.1647\n",
      "extra_boruta | thr=0.50 | F1_test=0.2433\n",
      "lgbm_boruta | thr=0.50 | F1_test=0.2547\n",
      "xgb_boruta | thr=0.34 | F1_test=0.1524\n",
      "cat_boruta | thr=0.50 | F1_test=0.2857\n",
      "logreg_correlation | thr=0.50 | F1_test=0.2115\n",
      "knn_correlation | thr=0.01 | F1_test=0.2143\n",
      "svm_correlation | thr=0.15 | F1_test=0.2119\n",
      "gnb_correlation | thr=0.15 | F1_test=0.2469\n",
      "dt_correlation | thr=0.47 | F1_test=0.2468\n",
      "rf_correlation | thr=0.49 | F1_test=0.2030\n",
      "ada_correlation | thr=0.41 | F1_test=0.2632\n",
      "gb_correlation | thr=0.28 | F1_test=0.1149\n",
      "extra_correlation | thr=0.50 | F1_test=0.2297\n",
      "lgbm_correlation | thr=0.50 | F1_test=0.2490\n",
      "xgb_correlation | thr=0.33 | F1_test=0.0808\n",
      "cat_correlation | thr=0.50 | F1_test=0.2335\n",
      "logreg_mi | thr=0.50 | F1_test=0.2240\n",
      "knn_mi | thr=0.01 | F1_test=0.1896\n",
      "svm_mi | thr=0.13 | F1_test=0.2617\n",
      "gnb_mi | thr=0.50 | F1_test=0.1143\n",
      "dt_mi | thr=0.49 | F1_test=0.2323\n",
      "rf_mi | thr=0.50 | F1_test=0.2136\n",
      "ada_mi | thr=0.38 | F1_test=0.2614\n",
      "gb_mi | thr=0.18 | F1_test=0.1304\n",
      "extra_mi | thr=0.50 | F1_test=0.2229\n",
      "lgbm_mi | thr=0.49 | F1_test=0.2382\n",
      "xgb_mi | thr=0.20 | F1_test=0.1094\n",
      "cat_mi | thr=0.50 | F1_test=0.2287\n",
      "logreg_rfe | thr=0.50 | F1_test=0.2324\n",
      "knn_rfe | thr=0.01 | F1_test=0.2019\n",
      "svm_rfe | thr=0.14 | F1_test=0.2055\n",
      "gnb_rfe | thr=0.50 | F1_test=0.1114\n",
      "dt_rfe | thr=0.46 | F1_test=0.2080\n",
      "rf_rfe | thr=0.50 | F1_test=0.2531\n",
      "ada_rfe | thr=0.41 | F1_test=0.2909\n",
      "gb_rfe | thr=0.29 | F1_test=0.2198\n",
      "extra_rfe | thr=0.50 | F1_test=0.2393\n",
      "lgbm_rfe | thr=0.50 | F1_test=0.2963\n",
      "xgb_rfe | thr=0.12 | F1_test=0.1967\n",
      "cat_rfe | thr=0.50 | F1_test=0.2472\n"
     ]
    }
   ],
   "execution_count": 537
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:32:30.193096Z",
     "start_time": "2026-01-08T03:32:30.173935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = results.sort_values(\n",
    "    by=results.columns.tolist(),\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "results"
   ],
   "id": "73c561e4742742",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             all    boruta  correlation        mi       rfe\n",
       "lgbm    0.282723  0.254658     0.248996  0.238227  0.296296\n",
       "ada     0.279070  0.313953     0.263158  0.261438  0.290909\n",
       "rf      0.273859  0.253846     0.203046  0.213592  0.253086\n",
       "cat     0.243243  0.285714     0.233503  0.228739  0.247191\n",
       "logreg  0.233062  0.228571     0.211494  0.223958  0.232432\n",
       "gb      0.222222  0.164706     0.114943  0.130435  0.219780\n",
       "extra   0.214493  0.243323     0.229730  0.222857  0.239316\n",
       "dt      0.202667  0.213740     0.246809  0.232295  0.208000\n",
       "xgb     0.192308  0.152381     0.080808  0.109375  0.196721\n",
       "knn     0.131868  0.207207     0.214286  0.189573  0.201923\n",
       "svm     0.110092  0.302752     0.211921  0.261682  0.205479\n",
       "gnb     0.107527  0.286885     0.246914  0.114286  0.111359"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>boruta</th>\n",
       "      <th>correlation</th>\n",
       "      <th>mi</th>\n",
       "      <th>rfe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.282723</td>\n",
       "      <td>0.254658</td>\n",
       "      <td>0.248996</td>\n",
       "      <td>0.238227</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>0.290909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.273859</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0.203046</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.253086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.233503</td>\n",
       "      <td>0.228739</td>\n",
       "      <td>0.247191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.233062</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.211494</td>\n",
       "      <td>0.223958</td>\n",
       "      <td>0.232432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.219780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra</th>\n",
       "      <td>0.214493</td>\n",
       "      <td>0.243323</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.222857</td>\n",
       "      <td>0.239316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.213740</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.232295</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.152381</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.196721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.207207</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.201923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.205479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gnb</th>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.286885</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.111359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 538
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hill-Climbing Ridge Ensemble",
   "id": "8a01d8ac677d5f08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:07:06.665642Z",
     "start_time": "2026-01-08T04:07:06.658978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_all_model_predictions(\n",
    "    MODELS, FEATURE_SETS,\n",
    "    X_train, y_train,\n",
    "    X_test\n",
    "):\n",
    "    train_preds = []\n",
    "    test_preds = []\n",
    "    model_names = []\n",
    "\n",
    "    for feature_name, feature_list in FEATURE_SETS.items():\n",
    "        X_train_sel = X_train[feature_list]\n",
    "        X_test_sel = X_test[feature_list]\n",
    "\n",
    "        for model_name in MODELS:\n",
    "            study_name = f\"{model_name}_{feature_name}_prob\"\n",
    "\n",
    "            with open(f\"models/{study_name}.pkl\", \"rb\") as f:\n",
    "                artifact = pickle.load(f)\n",
    "\n",
    "            model = artifact[\"model\"]\n",
    "\n",
    "            p_train = model.predict_proba(X_train_sel)[:, 1]\n",
    "            p_test = model.predict_proba(X_test_sel)[:, 1]\n",
    "\n",
    "            train_preds.append(p_train)\n",
    "            test_preds.append(p_test)\n",
    "            model_names.append(study_name)\n",
    "\n",
    "    P_train = np.column_stack(train_preds)\n",
    "    P_test = np.column_stack(test_preds)\n",
    "\n",
    "    return P_train, P_test, model_names\n"
   ],
   "id": "4ff282655725f62d",
   "outputs": [],
   "execution_count": 563
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:07:08.835054Z",
     "start_time": "2026-01-08T04:07:08.830032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensemble_f1_score(P, y, weights, threshold, alpha=0.01):\n",
    "    ensemble_proba = P @ weights\n",
    "    y_pred = (ensemble_proba >= threshold).astype(int)\n",
    "\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    ridge_penalty = alpha * np.sum(weights ** 2)\n",
    "\n",
    "    return f1 - ridge_penalty\n"
   ],
   "id": "7810182bde4e8498",
   "outputs": [],
   "execution_count": 564
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:07:11.975835Z",
     "start_time": "2026-01-08T04:07:11.970755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hill_climbing_ensemble(\n",
    "    P_train, y_train,\n",
    "    step=0.02,\n",
    "    max_iter=200,\n",
    "    alpha=0.01\n",
    "):\n",
    "    n_models = P_train.shape[1]\n",
    "    weights = np.ones(n_models) / n_models\n",
    "\n",
    "    best_threshold, _ = find_best_f1_threshold_dummy(P_train, y_train, weights)\n",
    "    best_score = ensemble_f1_score(P_train, y_train, weights, best_threshold, alpha)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        improved = False\n",
    "\n",
    "        for i in range(n_models):\n",
    "            for delta in [+step, -step]:\n",
    "                new_weights = weights.copy()\n",
    "                new_weights[i] += delta\n",
    "\n",
    "                if new_weights[i] < 0:\n",
    "                    continue\n",
    "\n",
    "                new_weights /= new_weights.sum()\n",
    "\n",
    "                thr, _ = find_best_f1_threshold_dummy(\n",
    "                    P_train, y_train, new_weights\n",
    "                )\n",
    "\n",
    "                score = ensemble_f1_score(\n",
    "                    P_train, y_train, new_weights, thr, alpha\n",
    "                )\n",
    "\n",
    "                if score > best_score:\n",
    "                    weights = new_weights\n",
    "                    best_score = score\n",
    "                    best_threshold = thr\n",
    "                    improved = True\n",
    "\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "    return weights, best_threshold, best_score\n"
   ],
   "id": "1548889091e8b535",
   "outputs": [],
   "execution_count": 565
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:07:14.729148Z",
     "start_time": "2026-01-08T04:07:14.722622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_best_f1_threshold_dummy(P, y, weights, thresholds=np.linspace(0.01, 0.5, 50)):\n",
    "    ensemble_proba = P @ weights\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (ensemble_proba >= t).astype(int)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = t\n",
    "\n",
    "    return best_thr, best_f1\n"
   ],
   "id": "3aa9f34af366a9ee",
   "outputs": [],
   "execution_count": 566
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:07:27.491639Z",
     "start_time": "2026-01-08T04:07:16.168942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P_train, P_test, model_names = load_all_model_predictions(\n",
    "    MODELS, FEATURE_SETS,\n",
    "    X_train, y_train,\n",
    "    X_test\n",
    ")\n",
    "\n",
    "weights, best_thr, train_score = hill_climbing_ensemble(\n",
    "    P_train, y_train,\n",
    "    step=0.02,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "y_test_pred = (P_test @ weights >= best_thr).astype(int)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(\"----------------\")\n",
    "print(f\"F1 train: {train_score:.4f}\")\n",
    "print(f\"F1 test : {f1_test:.4f}\")\n",
    "print(f\"Threshold: {best_thr:.3f}\")\n",
    "\n",
    "for name, w in sorted(zip(model_names, weights), key=lambda x: -x[1]):\n",
    "    if w > 0.01:\n",
    "        print(f\"{name:40s}  weight={w:.3f}\")\n",
    "\n",
    "os.makedirs(\"models/hc\", exist_ok=True)\n",
    "\n",
    "hc_path = \"models/hc/hc_ensemble.pkl\"\n",
    "\n",
    "hc_artifact = {\n",
    "    \"type\": \"hill_climbing_ensemble\",\n",
    "    \"weights\": weights,\n",
    "    \"threshold\": best_thr,\n",
    "    \"model_names\": model_names,\n",
    "    \"f1_train\": train_score\n",
    "}\n",
    "\n",
    "with open(hc_path, \"wb\") as f:\n",
    "    pickle.dump(hc_artifact, f)"
   ],
   "id": "b71383ba82ae6c41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE RESULTS\n",
      "----------------\n",
      "F1 train: 0.9972\n",
      "F1 test : 0.1081\n",
      "Threshold: 0.460\n",
      "svm_all_prob                              weight=0.058\n",
      "knn_all_prob                              weight=0.057\n",
      "knn_rfe_prob                              weight=0.043\n",
      "xgb_correlation_prob                      weight=0.042\n",
      "gb_correlation_prob                       weight=0.041\n",
      "gb_boruta_prob                            weight=0.040\n",
      "gb_all_prob                               weight=0.039\n",
      "knn_correlation_prob                      weight=0.029\n",
      "ada_rfe_prob                              weight=0.029\n",
      "svm_rfe_prob                              weight=0.028\n",
      "dt_mi_prob                                weight=0.028\n",
      "cat_rfe_prob                              weight=0.026\n",
      "svm_boruta_prob                           weight=0.026\n",
      "xgb_rfe_prob                              weight=0.025\n",
      "lgbm_rfe_prob                             weight=0.025\n",
      "gb_rfe_prob                               weight=0.025\n",
      "xgb_all_prob                              weight=0.022\n",
      "knn_boruta_prob                           weight=0.019\n"
     ]
    }
   ],
   "execution_count": 567
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summary",
   "id": "c6642b719e03e9f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:19:32.426683Z",
     "start_time": "2026-01-08T04:19:32.419487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    return y_pred, proba\n",
    "\n",
    "def evaluate_model(model, X, y, threshold=0.5):\n",
    "    y_pred, proba = predict_with_threshold(model, X, threshold)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y, y_pred),\n",
    "        \"f1\": f1_score(y, y_pred),\n",
    "        \"avg_precision\": average_precision_score(y, proba)\n",
    "    }\n",
    "\n",
    "def evaluate_saved_models(\n",
    "    model_paths,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for name, path in model_paths.items():\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            artifact = pickle.load(f)\n",
    "\n",
    "        if isinstance(artifact, dict) and artifact.get(\"type\") == \"hill_climbing_ensemble\":\n",
    "\n",
    "            weights = artifact[\"weights\"]\n",
    "            threshold = artifact[\"threshold\"]\n",
    "\n",
    "            P_train, P_test, _ = load_all_model_predictions(\n",
    "                MODELS, FEATURE_SETS,\n",
    "                X_train, y_train,\n",
    "                X_test\n",
    "            )\n",
    "\n",
    "            train_proba = P_train @ weights\n",
    "            train_pred = (train_proba >= threshold).astype(int)\n",
    "\n",
    "            test_proba = P_test @ weights\n",
    "            test_pred = (test_proba >= threshold).astype(int)\n",
    "\n",
    "            train_scores = {\n",
    "                \"accuracy\": accuracy_score(y_train, train_pred),\n",
    "                \"f1\": f1_score(y_train, train_pred),\n",
    "                \"avg_precision\": average_precision_score(y_train, train_proba)\n",
    "            }\n",
    "\n",
    "            test_scores = {\n",
    "                \"accuracy\": accuracy_score(y_test, test_pred),\n",
    "                \"f1\": f1_score(y_test, test_pred),\n",
    "                \"avg_precision\": average_precision_score(y_test, test_proba)\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            if isinstance(artifact, dict):\n",
    "                model = artifact[\"model\"]\n",
    "                threshold = artifact.get(\"threshold\", 0.5)\n",
    "            else:\n",
    "                model = artifact\n",
    "                threshold = 0.5\n",
    "\n",
    "            train_scores = evaluate_model(\n",
    "                model, X_train, y_train, threshold\n",
    "            )\n",
    "\n",
    "            test_scores = evaluate_model(\n",
    "                model, X_test, y_test, threshold\n",
    "            )\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "\n",
    "            \"train_f1\": train_scores[\"f1\"],\n",
    "            \"test_f1\": test_scores[\"f1\"],\n",
    "\n",
    "            \"train_accuracy\": train_scores[\"accuracy\"],\n",
    "            \"test_accuracy\": test_scores[\"accuracy\"],\n",
    "\n",
    "            \"train_avg_precision\": train_scores[\"avg_precision\"],\n",
    "            \"test_avg_precision\": test_scores[\"avg_precision\"],\n",
    "\n",
    "            \"threshold\": threshold\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"model\")\n",
    "\n",
    "    return df.sort_values(\"test_avg_precision\", ascending=False)"
   ],
   "id": "7dc1ee424bd02ad",
   "outputs": [],
   "execution_count": 574
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T04:19:35.938855Z",
     "start_time": "2026-01-08T04:19:35.783133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PATHS = {\n",
    "    \"rf_all\": \"models/prob_f1/rf_all_prob.pkl\",\n",
    "    \"xgb_all\": \"models/prob_f1/xgb_all_prob.pkl\",\n",
    "    \"cat_all\": \"models/prob_f1/cat_all_prob.pkl\",\n",
    "    \"ensemble_hill\": \"models/hc/hc_ensemble.pkl\"\n",
    "}\n",
    "\n",
    "results = evaluate_saved_models(\n",
    "    MODEL_PATHS,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "results"
   ],
   "id": "a985cbd61ec6a351",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/logreg_all_prob.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[575]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      1\u001B[39m MODEL_PATHS = {\n\u001B[32m      2\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mrf_all\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmodels/prob_f1/rf_all_prob.pkl\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mxgb_all\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmodels/prob_f1/xgb_all_prob.pkl\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcat_all\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmodels/prob_f1/cat_all_prob.pkl\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      5\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mensemble_hill\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmodels/hc/hc_ensemble.pkl\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      6\u001B[39m }\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m results = \u001B[43mevaluate_saved_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mMODEL_PATHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m results\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[574]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mevaluate_saved_models\u001B[39m\u001B[34m(model_paths, X_train, y_train, X_test, y_test)\u001B[39m\n\u001B[32m     29\u001B[39m weights = artifact[\u001B[33m\"\u001B[39m\u001B[33mweights\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     30\u001B[39m threshold = artifact[\u001B[33m\"\u001B[39m\u001B[33mthreshold\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m P_train, P_test, _ = \u001B[43mload_all_model_predictions\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mMODELS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFEATURE_SETS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_test\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m train_proba = P_train @ weights\n\u001B[32m     39\u001B[39m train_pred = (train_proba >= threshold).astype(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[563]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mload_all_model_predictions\u001B[39m\u001B[34m(MODELS, FEATURE_SETS, X_train, y_train, X_test)\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m model_name \u001B[38;5;129;01min\u001B[39;00m MODELS:\n\u001B[32m     15\u001B[39m     study_name = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfeature_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_prob\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodels/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mstudy_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.pkl\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     18\u001B[39m         artifact = pickle.load(f)\n\u001B[32m     20\u001B[39m     model = artifact[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/3.11/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:344\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    338\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    339\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    340\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    341\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    342\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m344\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'models/logreg_all_prob.pkl'"
     ]
    }
   ],
   "execution_count": 575
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Wnioski:\n",
    "- Oversampling poprzez imbalanced learn\n",
    "- Wagi bardzo wany hiperparametr\n",
    "- Avarage precision score zamiast f1 dla recall > precision importance ale nie zbalansowane klasy\n",
    "- Dobry ensembling zawsze polepsza wyniki"
   ],
   "id": "132efcfa9ddcba2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "todo:\n",
    "- ustawic zeby foldery z zapisanymi modelami byly kompatybilne do dynamicznego uzytku\n",
    "- liczyc fb accuracy\n",
    "- wytrenowac modele f1 oversampling i fb oversampling"
   ],
   "id": "f9101ffa15d5757b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
